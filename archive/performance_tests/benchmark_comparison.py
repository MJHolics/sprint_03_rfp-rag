"""
ì„±ëŠ¥ ê°œì„  ë‹¨ê³„ë³„ ë¹„êµ ë²¤ì¹˜ë§ˆí¬
- íŒ¨ì¹˜ ì „ (ê¸°ë³¸)
- 1ë‹¨ê³„ íŒ¨ì¹˜ í›„ (SQLite ìµœì í™”, ìºì‹±, ë°°ì¹˜ ì²˜ë¦¬)
- 2ë‹¨ê³„ íŒ¨ì¹˜ í›„ (ë³‘ë ¬ ì²˜ë¦¬, ë²¡í„° ìµœì í™”, ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë°)
"""
import time
import psutil
import os
import sqlite3
from pathlib import Path
import sys
import shutil
import tempfile

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python íŒ¨ìŠ¤ì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.rag_system import RAGSystem
from config.settings import *

class PerformanceBenchmark:
    def __init__(self):
        self.results = {
            'baseline': {},  # íŒ¨ì¹˜ ì „
            'stage1': {},    # 1ë‹¨ê³„ í›„
            'stage2': {}     # 2ë‹¨ê³„ í›„
        }

    def measure_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (MB)"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024

    def measure_db_performance(self, db_path):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •"""
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
            cursor.execute("SELECT name FROM sqlite_master WHERE type='index'")
            indexes = [row[0] for row in cursor.fetchall()]

            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
            queries = [
                "SELECT COUNT(*) FROM documents",
                "SELECT * FROM documents WHERE agency LIKE '%êµ­ë¯¼ì—°ê¸ˆ%' LIMIT 10",
                "SELECT * FROM documents WHERE budget != '' LIMIT 10",
                "SELECT agency, COUNT(*) FROM documents GROUP BY agency LIMIT 10"
            ]

            query_times = []
            for query in queries:
                start_time = time.time()
                try:
                    cursor.execute(query)
                    cursor.fetchall()
                    query_times.append(time.time() - start_time)
                except Exception as e:
                    query_times.append(999)  # ì˜¤ë¥˜ ì‹œ ë†’ì€ ê°’

            conn.close()

            return {
                'total_indexes': len(indexes),
                'optimized_indexes': len([idx for idx in indexes if 'agency' in idx or 'budget' in idx]),
                'avg_query_time': sum(query_times) / len(query_times),
                'query_times': query_times
            }
        except Exception as e:
            return {'error': str(e)}

    def test_document_processing(self, test_mode="current"):
        """ë¬¸ì„œ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {test_mode.upper()} ëª¨ë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        print(f"{'='*60}")

        # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ë°ì´í„° ì˜í–¥ ë°©ì§€)
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_vector_db = os.path.join(temp_dir, "test_vector_db")
            temp_metadata_db = os.path.join(temp_dir, "test_metadata.db")

            # í…ŒìŠ¤íŠ¸ìš© RAG ì‹œìŠ¤í…œ ìƒì„±
            if test_mode == "baseline":
                # íŒ¨ì¹˜ ì „ ëª¨ë“œ (ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™”, ê¸°ë³¸ ì„¤ì •)
                rag_system = self._create_baseline_system(temp_vector_db, temp_metadata_db)
            else:
                # í˜„ì¬ ëª¨ë“œ (ëª¨ë“  ìµœì í™” ì ìš©)
                rag_system = RAGSystem(
                    vector_db_path=temp_vector_db,
                    metadata_db_path=temp_metadata_db,
                    chunk_size=CHUNK_SIZE,
                    overlap=CHUNK_OVERLAP
                )

            # ì„±ëŠ¥ ì¸¡ì •
            initial_memory = self.measure_memory_usage()
            start_time = time.time()

            # ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ë¹„êµë¥¼ ìœ„í•´)
            test_files = list(Path("./files").glob("*.pdf"))[:5] + list(Path("./files").glob("*.hwp"))[:5]

            results = {
                'total_files': len(test_files),
                'successful': 0,
                'failed': 0,
                'total_chunks': 0,
                'processing_time': 0,
                'memory_used': 0,
                'files_per_minute': 0,
                'chunks_per_second': 0
            }

            if not test_files:
                print(" í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return results

            # ê°œë³„ íŒŒì¼ ì²˜ë¦¬
            for file_path in test_files:
                try:
                    result = rag_system.process_document(str(file_path))
                    if result.success:
                        results['successful'] += 1
                        results['total_chunks'] += result.total_chunks
                    else:
                        results['failed'] += 1
                except Exception:
                    results['failed'] += 1

            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            processing_time = time.time() - start_time
            final_memory = self.measure_memory_usage()

            results.update({
                'processing_time': processing_time,
                'memory_used': final_memory - initial_memory,
                'files_per_minute': (results['successful'] / processing_time) * 60 if processing_time > 0 else 0,
                'chunks_per_second': results['total_chunks'] / processing_time if processing_time > 0 else 0
            })

            # DB ì„±ëŠ¥ ì¸¡ì •
            db_performance = self.measure_db_performance(temp_metadata_db)
            results['db_performance'] = db_performance

            print(f"ğŸ“ ì²˜ë¦¬ ê²°ê³¼:")
            print(f"   ì´ íŒŒì¼: {results['total_files']}")
            print(f"   ì„±ê³µ: {results['successful']}")
            print(f"   ì‹¤íŒ¨: {results['failed']}")
            print(f"   ì´ ì²­í¬: {results['total_chunks']}")
            print(f" ì„±ëŠ¥ ì§€í‘œ:")
            print(f"   ì²˜ë¦¬ ì‹œê°„: {results['processing_time']:.2f}ì´ˆ")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©: {results['memory_used']:.1f}MB")
            print(f"   ì²˜ë¦¬ ì†ë„: {results['files_per_minute']:.1f} íŒŒì¼/ë¶„")
            print(f"   ì²­í‚¹ ì†ë„: {results['chunks_per_second']:.1f} ì²­í¬/ì´ˆ")
            print(f"ğŸ’¾ DB ì„±ëŠ¥:")
            print(f"   ì¸ë±ìŠ¤ ìˆ˜: {db_performance.get('total_indexes', 0)}")
            print(f"   ìµœì í™” ì¸ë±ìŠ¤: {db_performance.get('optimized_indexes', 0)}")
            print(f"   í‰ê·  ì¿¼ë¦¬ ì‹œê°„: {db_performance.get('avg_query_time', 0):.3f}ì´ˆ")

            return results

    def _create_baseline_system(self, vector_db_path, metadata_db_path):
        """íŒ¨ì¹˜ ì „ ì‹œìŠ¤í…œ ì‹œë®¬ë ˆì´ì…˜"""
        # ê¸°ë³¸ RAG ì‹œìŠ¤í…œ ìƒì„±í•˜ë˜ ë³‘ë ¬ ì²˜ë¦¬ ë“± ìµœì í™” ë¹„í™œì„±í™”
        # ì‹¤ì œë¡œëŠ” í˜„ì¬ ì‹œìŠ¤í…œì´ì§€ë§Œ ìˆœì°¨ ì²˜ë¦¬ë¡œ ì œí•œ
        rag_system = RAGSystem(vector_db_path, metadata_db_path, CHUNK_SIZE, CHUNK_OVERLAP)

        # ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™” (process_directory ë©”ì„œë“œ íŒ¨ì¹˜)
        original_process_directory = rag_system.process_directory

        def sequential_process_directory(directory_path, metadata_csv_path=None):
            """ìˆœì°¨ ì²˜ë¦¬ ë²„ì „"""
            from pathlib import Path
            import time

            directory_path = Path(directory_path)
            results = {
                'total_files': 0, 'successful': 0, 'failed': 0,
                'total_chunks': 0, 'processing_time': 0, 'errors': []
            }

            start_time = time.time()
            supported_files = []
            for ext in rag_system.processors.keys():
                supported_files.extend(directory_path.glob(f"**/*{ext}"))

            results['total_files'] = len(supported_files)

            # ìˆœì°¨ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ ì—†ìŒ)
            for file_path in supported_files:
                try:
                    result = rag_system.process_document(str(file_path))
                    if result.success:
                        results['successful'] += 1
                        results['total_chunks'] += result.total_chunks
                    else:
                        results['failed'] += 1
                        results['errors'].append({
                            'file': file_path.name,
                            'error': result.error_message
                        })
                except Exception as e:
                    results['failed'] += 1
                    results['errors'].append({
                        'file': file_path.name,
                        'error': str(e)
                    })

            results['processing_time'] = time.time() - start_time
            return results

        rag_system.process_directory = sequential_process_directory
        return rag_system

    def test_search_performance(self, num_queries=5):
        """ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ” ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        print(f"{'='*40}")

        # í˜„ì¬ ì‹œìŠ¤í…œìœ¼ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        rag_system = RAGSystem(
            vector_db_path=str(VECTOR_DB_PATH),
            metadata_db_path=str(METADATA_DB_PATH),
            chunk_size=CHUNK_SIZE,
            overlap=CHUNK_OVERLAP
        )

        test_queries = [
            "ì‹œìŠ¤í…œ êµ¬ì¶• ì˜ˆì‚°",
            "í”„ë¡œì íŠ¸ ê¸°ê°„",
            "ê°œë°œ ì¸ë ¥",
            "ê¸°ìˆ  ìš”êµ¬ì‚¬í•­",
            "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨"
        ][:num_queries]

        total_time = 0
        total_confidence = 0
        successful_searches = 0

        for i, query in enumerate(test_queries, 1):
            try:
                start_time = time.time()
                result = rag_system.search_and_answer(query, top_k=3)
                search_time = time.time() - start_time

                total_time += search_time
                total_confidence += result['confidence']
                successful_searches += 1

                print(f"{i}. '{query}' - {search_time:.2f}ì´ˆ (ì‹ ë¢°ë„: {result['confidence']:.3f})")

            except Exception as e:
                print(f"{i}. '{query}' - ì‹¤íŒ¨: {e}")

        avg_time = total_time / successful_searches if successful_searches > 0 else 0
        avg_confidence = total_confidence / successful_searches if successful_searches > 0 else 0

        return {
            'avg_response_time': avg_time,
            'avg_confidence': avg_confidence,
            'total_queries': len(test_queries),
            'successful_queries': successful_searches
        }

    def compare_performance(self):
        """ì „ì²´ ì„±ëŠ¥ ë¹„êµ"""
        print("ğŸš€ RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„  ë‹¨ê³„ë³„ ë¹„êµ")
        print("="*80)

        # 1. íŒ¨ì¹˜ ì „ (ê¸°ë³¸) í…ŒìŠ¤íŠ¸
        print("\n íŒ¨ì¹˜ ì „ (ê¸°ë³¸) ì„±ëŠ¥ ì¸¡ì •")
        baseline_results = self.test_document_processing("baseline")
        self.results['baseline'] = baseline_results

        # 2. í˜„ì¬ (2ë‹¨ê³„ íŒ¨ì¹˜ í›„) í…ŒìŠ¤íŠ¸
        print("\n 2ë‹¨ê³„ íŒ¨ì¹˜ í›„ ì„±ëŠ¥ ì¸¡ì •")
        stage2_results = self.test_document_processing("stage2")
        self.results['stage2'] = stage2_results

        # 3. ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        search_results = self.test_search_performance()

        # 4. ê²°ê³¼ ë¹„êµ ë° ìš”ì•½
        self.print_comparison_summary(search_results)

    def print_comparison_summary(self, search_results):
        """ì„±ëŠ¥ ë¹„êµ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“ˆ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ë¶„ì„")
        print("="*80)

        baseline = self.results['baseline']
        stage2 = self.results['stage2']

        # ì²˜ë¦¬ ì†ë„ ê°œì„ 
        if baseline['processing_time'] > 0 and stage2['processing_time'] > 0:
            speed_improvement = baseline['processing_time'] / stage2['processing_time']
            print(f" ë¬¸ì„œ ì²˜ë¦¬ ì†ë„ ê°œì„ : {speed_improvement:.1f}ë°°")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
        if baseline['memory_used'] > 0 and stage2['memory_used'] > 0:
            memory_improvement = (baseline['memory_used'] - stage2['memory_used']) / baseline['memory_used'] * 100
            print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°œì„ : {memory_improvement:.1f}% ì ˆì•½")

        # ì²˜ë¦¬ëŸ‰ ë¹„êµ
        baseline_throughput = baseline.get('files_per_minute', 0)
        stage2_throughput = stage2.get('files_per_minute', 0)
        if baseline_throughput > 0:
            throughput_improvement = stage2_throughput / baseline_throughput
            print(f"ğŸ“Š ì²˜ë¦¬ëŸ‰ ê°œì„ : {throughput_improvement:.1f}ë°°")

        # DB ì„±ëŠ¥ ë¹„êµ
        baseline_db = baseline.get('db_performance', {})
        stage2_db = stage2.get('db_performance', {})

        baseline_query_time = baseline_db.get('avg_query_time', 0)
        stage2_query_time = stage2_db.get('avg_query_time', 0)

        if baseline_query_time > 0 and stage2_query_time > 0:
            db_improvement = baseline_query_time / stage2_query_time
            print(f"ğŸ—„ï¸ DB ì¿¼ë¦¬ ì†ë„ ê°œì„ : {db_improvement:.1f}ë°°")

        # ê²€ìƒ‰ ì„±ëŠ¥
        print(f"ğŸ” ê²€ìƒ‰ ì„±ëŠ¥:")
        print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {search_results['avg_response_time']:.2f}ì´ˆ")
        print(f"   í‰ê·  ì‹ ë¢°ë„: {search_results['avg_confidence']:.3f}")
        print(f"   ì„±ê³µë¥ : {search_results['successful_queries']}/{search_results['total_queries']}")

        print(f"\nğŸ“‹ ìƒì„¸ ë¹„êµí‘œ:")
        print(f"{'ì§€í‘œ':<15} {'íŒ¨ì¹˜ ì „':<12} {'2ë‹¨ê³„ í›„':<12} {'ê°œì„  íš¨ê³¼':<12}")
        print("-" * 60)
        print(f"{'ì²˜ë¦¬ì‹œê°„(ì´ˆ)':<15} {baseline['processing_time']:<12.2f} {stage2['processing_time']:<12.2f} {speed_improvement:<12.1f}ë°°")
        print(f"{'ë©”ëª¨ë¦¬(MB)':<15} {baseline['memory_used']:<12.1f} {stage2['memory_used']:<12.1f} {memory_improvement:<12.1f}% ì ˆì•½")
        print(f"{'íŒŒì¼/ë¶„':<15} {baseline_throughput:<12.1f} {stage2_throughput:<12.1f} {throughput_improvement:<12.1f}ë°°")
        print(f"{'ì¿¼ë¦¬ì‹œê°„(ì´ˆ)':<15} {baseline_query_time:<12.3f} {stage2_query_time:<12.3f} {db_improvement:<12.1f}ë°°")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    if not Path("./files").exists():
        print(" ./files ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
        return

    benchmark = PerformanceBenchmark()
    benchmark.compare_performance()

if __name__ == "__main__":
    main()